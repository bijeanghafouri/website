blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::preview_site(startup = TRUE)
blogdown:::preview_site()
blogdown:::preview_site()
blogdown:::serve_site()
blogdown:::preview_site(startup = TRUE)
blogdown:::preview_site(startup = TRUE)
blogdown:::preview_site()
blogdown:::preview_site(startup = TRUE)
blogdown:::serve_site()
file.create('netlify.toml')
blogdown::hugo_version()
blogdown:::preview_site(startup = TRUE)
hugo new  --kind post post/my-article-name
hugo new --king post post/example_article
hugo new --kind post post/example_article
blogdown:::preview_site(startup = TRUE)
blogdown:::new_post_addin()
blogdown:::preview_site(startup = TRUE)
blogdown:::preview_site(startup = TRUE)
setwd("~/Code/psyc625/Homeworks/assignments/4")
perceptron <- function(x,y){
## Set the parameters
weight <- rep(0, ncol(x) + 1)     # weights
correct_count <- 0     # count of correct predictions
total_iterations <- 1    # count total iterations
result <- list()     # store result for output
k <- 0     # count total number of loops
## Start iteration
while(correct_count < (nrow(x) + 1)){
for (rowe in 1:nrow(x)){
# Prediction
y_pred <- weight[1]*x[rowe, 1] + weight[2]*x[rowe, 2] + weight[3]*1
y_pred <- sum(weight[1:ncol(x)] * x[rowe, 1:ncol(x)]) + weight[ncol(x) + 1]*1
# Classify Prediction (1 or 0)
if(y_pred < 1){
y_pred <- 0
} else{y_pred <- 1}
# If mistake is made, update weights
if((y_pred != (y[rowe])) == TRUE){
#  weight[1] <- weight[1] + 1*(y[rowe] - y_pred)*x[rowe, 1]
#  weight[2] <- weight[2] + 1*(y[rowe] - y_pred)*x[rowe, 2]
#  weight[3] <- weight[3] + 1*(y[rowe] - y_pred)*1
# reset count of correct predictions because mistake is made
correct_count <- 0
## automatic update rule
# feature weights:
weight[1:ncol(x)] <-   weight[1:ncol(x)] +  1*(y[rowe] - y_pred)*x[rowe, 1:ncol(x)]
# bias weight:
weight[ncol(x) + 1] <- weight[ncol(x) + 1] + 1*(y[rowe] - y_pred)*1
}
# If no mistake, update count of correct predictions
else{ correct_count <- correct_count + 1 }
# Count total iterations
total_iterations <- total_iterations + 1
# Error call: convergence too high
if(total_iterations > 1000){
return('Woah mate, 1000 iterations? I known I am quick, but that is too many.')
}
# Count iterations. Counter adds each time loop reaches 1.
if((rowe == 1) == TRUE){ k <- k + 1 }
# When count of correct predictions reaches total number of rows (full loop of no errors)
if((correct_count == nrow(x)) == TRUE){
# select weights from x's
result$w <- weight[1:ncol(x)]
# select bias weight
result$w_0 <-  weight[-(1:ncol(x))]
# select iteration
result$k <- k
return(result)
}
} # for loop
} # while loop
}
# Test 1
x <- array (data=c(0, 0, 1, 1, 0, 1, 0, 1), c(4,2))
y<- c(0,0,0,1)
perceptron (x,y)
# Test 2
x <- array (data=c(0, 0, 1, 1, 0, 1, 0, 1), c(4,2))
y<- c(0,1,1,1)
perceptron(x,y)
# Test 3
x <- array (data=c(0, 0, 1, 1, 0, 1, 0, 1), c(4,2))
y<- c(0,1,1,0)
perceptron (x,y)
# Test
simulateData <- function (n, p, meanGroup1, meanGroup2, sd){
npos <- round(n/2)
nneg <- n-npos
xpos <- matrix(rnorm(npos*p,mean=meanGroup1,sd=sd),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanGroup2,sd=sd),npos,p)
x <- rbind(xpos,xneg)
y <- matrix(c(rep(1,npos),rep(0,nneg)))
all.data <- cbind(y,x)
return(all.data)
}
n <- 100
all.data <- simulateData(n, 2, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 5, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 2, 3, 1, 1)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 2, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 5, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 2, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 2, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 2, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
perceptron <- function(x, y, n = 1){
## Set the parameters
weight <- rep(0, ncol(x) + 1)     # weights
correct_count <- 0     # count of correct predictions
total_iterations <- 1    # count total iterations
result <- list()     # store result for output
k <- 0     # count total number of loops
## Start iteration
while(correct_count < (nrow(x) + 1)){
for (rowe in 1:nrow(x)){
# Prediction
y_pred <- sum(weight[1:ncol(x)] * x[rowe, 1:ncol(x)]) + weight[ncol(x) + 1]*1
# Classify Prediction (1 or 0)
if(y_pred < 1){
y_pred <- 0
} else{y_pred <- 1}
# If mistake is made, update weights
if((y_pred != (y[rowe])) == TRUE){
correct_count <- 0
## automatic update rule
# feature weights:
weight[1:ncol(x)] <-   weight[1:ncol(x)] +  n*(y[rowe] - y_pred)*x[rowe, 1:ncol(x)]
# bias weight:
weight[ncol(x) + 1] <- weight[ncol(x) + 1] + n*(y[rowe] - y_pred)*1
}
# If no mistake, update count of correct predictions
else{ correct_count <- correct_count + 1 }
# Count total iterations
total_iterations <- total_iterations + 1
# Error call: convergence too high
if(total_iterations > 1000){
return('Woah mate, 1000 iterations? I known I am quick, but that is too many.')
}
# Count iterations. Counter adds each time loop reaches 1.
if((rowe == 1) == TRUE){ k <- k + 1 }
# When count of correct predictions reaches total number of rows (full loop of no errors)
if((correct_count == nrow(x)) == TRUE){
# select weights from x's
result$w <- weight[1:ncol(x)]
# select bias weight
result$w_0 <-  weight[-(1:ncol(x))]
# select iteration
result$k <- k
# return result
return(result)
}
} # for loop
} # while loop
} # end of function
# Test 1
x <- array (data=c(0, 0, 1, 1, 0, 1, 0, 1), c(4,2))
y<- c(0,0,0,1)
perceptron (x,y)
# Test 2
x <- array (data=c(0, 0, 1, 1, 0, 1, 0, 1), c(4,2))
y<- c(0,1,1,1)
perceptron(x,y)
# Test 3
x <- array (data=c(0, 0, 1, 1, 0, 1, 0, 1), c(4,2))
y<- c(0,1,1,0)
perceptron (x,y)
# Test
simulateData <- function (n, p, meanGroup1, meanGroup2, sd){
npos <- round(n/2)
nneg <- n-npos
xpos <- matrix(rnorm(npos*p,mean=meanGroup1,sd=sd),npos,p)
xneg <- matrix(rnorm(nneg*p,mean=meanGroup2,sd=sd),npos,p)
x <- rbind(xpos,xneg)
y <- matrix(c(rep(1,npos),rep(0,nneg)))
all.data <- cbind(y,x)
return(all.data)
}
n <- 100
all.data <- simulateData(n, 2, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 5, 3, 1, 0.5)
perceptron(all.data[,-1],all.data[,1])
all.data <- simulateData(n, 2, 3, 1, 1)
perceptron(all.data[,-1],all.data[,1])
perceptron (x,y, n = 2)
# Test 1
x <- array (data=c(0, 0, 1, 1, 0, 1, 0, 1), c(4,2))
y<- c(0,0,0,1)
perceptron (x,y, n = 2)
perceptron (x,y, n = 2)
perceptron (x,y, n = 1)
perceptron (x,y, n = 0.001)
perceptron (x,y, n = 0.01)
perceptron (x,y, n = 2)
perceptron (x,y, n = 5)
perceptron (x,y, n = 0.8)
perceptron (x,y, n = 0.1)
perceptron (x,y, n = 0.01)
perceptron (x,y, n = 0.001)
blogdown:::preview_site(startup = TRUE)
